{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://www.lawrence.edu/fast/greggj/CMSC490/github.zip\n",
    "#!unzip github.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 15:20:21.301964: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-17 15:20:21.325330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,  cross_val_predict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(df,col,clean_col):\n",
    "    \n",
    "    def general(data):\n",
    "        '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "        and remove words containing numbers'''\n",
    "        data = data.lower()\n",
    "        data = re.sub('\\[.*?\\]', '', data)\n",
    "        data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "        data = re.sub('<.*?>+', '', data)\n",
    "        data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "        data = re.sub('\\n', '', data)\n",
    "        data = re.sub('\\w*\\d\\w*', '', data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def remove_contractions(data):\n",
    "        data = re.sub(r\"he's\", \"he is\", data)\n",
    "        data = re.sub(r\"there's\", \"there is\", data)\n",
    "        data = re.sub(r\"We're\", \"We are\", data)\n",
    "        data = re.sub(r\"That's\", \"That is\", data)\n",
    "        data = re.sub(r\"won't\", \"will not\", data)\n",
    "        data = re.sub(r\"they're\", \"they are\", data)\n",
    "        data = re.sub(r\"Can't\", \"Cannot\", data)\n",
    "        data = re.sub(r\"wasn't\", \"was not\", data)\n",
    "        data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n",
    "        data= re.sub(r\"aren't\", \"are not\", data)\n",
    "        data = re.sub(r\"isn't\", \"is not\", data)\n",
    "        data = re.sub(r\"What's\", \"What is\", data)\n",
    "        data = re.sub(r\"haven't\", \"have not\", data)\n",
    "        data = re.sub(r\"hasn't\", \"has not\", data)\n",
    "        data = re.sub(r\"There's\", \"There is\", data)\n",
    "        data = re.sub(r\"He's\", \"He is\", data)\n",
    "        data = re.sub(r\"It's\", \"It is\", data)\n",
    "        data = re.sub(r\"You're\", \"You are\", data)\n",
    "        data = re.sub(r\"I'M\", \"I am\", data)\n",
    "        data = re.sub(r\"shouldn't\", \"should not\", data)\n",
    "        data = re.sub(r\"wouldn't\", \"would not\", data)\n",
    "        data = re.sub(r\"i'm\", \"I am\", data)\n",
    "        data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n",
    "        data = re.sub(r\"I'm\", \"I am\", data)\n",
    "        data = re.sub(r\"Isn't\", \"is not\", data)\n",
    "        data = re.sub(r\"Here's\", \"Here is\", data)\n",
    "        data = re.sub(r\"you've\", \"you have\", data)\n",
    "        data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n",
    "        data = re.sub(r\"we're\", \"we are\", data)\n",
    "        data = re.sub(r\"what's\", \"what is\", data)\n",
    "        data = re.sub(r\"couldn't\", \"could not\", data)\n",
    "        data = re.sub(r\"we've\", \"we have\", data)\n",
    "        data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n",
    "        data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n",
    "        data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n",
    "        data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n",
    "        data = re.sub(r\"who's\", \"who is\", data)\n",
    "        data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n",
    "        data = re.sub(r\"y'all\", \"you all\", data)\n",
    "        data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n",
    "        data = re.sub(r\"would've\", \"would have\", data)\n",
    "        data = re.sub(r\"it'll\", \"it will\", data)\n",
    "        data = re.sub(r\"we'll\", \"we will\", data)\n",
    "        data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n",
    "        data = re.sub(r\"We've\", \"We have\", data)\n",
    "        data = re.sub(r\"he'll\", \"he will\", data)\n",
    "        data = re.sub(r\"Y'all\", \"You all\", data)\n",
    "        data = re.sub(r\"Weren't\", \"Were not\", data)\n",
    "        data = re.sub(r\"Didn't\", \"Did not\", data)\n",
    "        data = re.sub(r\"they'll\", \"they will\", data)\n",
    "        data = re.sub(r\"they'd\", \"they would\", data)\n",
    "        data = re.sub(r\"DON'T\", \"DO NOT\", data)\n",
    "        data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n",
    "        data = re.sub(r\"they've\", \"they have\", data)\n",
    "        data = re.sub(r\"i'd\", \"I would\", data)\n",
    "        data = re.sub(r\"should've\", \"should have\", data)\n",
    "        data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n",
    "        data = re.sub(r\"where's\", \"where is\", data)\n",
    "        data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n",
    "        data = re.sub(r\"we'd\", \"we would\", data)\n",
    "        data = re.sub(r\"i'll\", \"I will\", data)\n",
    "        data = re.sub(r\"weren't\", \"were not\", data)\n",
    "        data = re.sub(r\"They're\", \"They are\", data)\n",
    "        data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n",
    "        data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n",
    "        data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n",
    "        data = re.sub(r\"let's\", \"let us\", data)\n",
    "        data = re.sub(r\"it's\", \"it is\", data)\n",
    "        data = re.sub(r\"can't\", \"cannot\", data)\n",
    "        data = re.sub(r\"don't\", \"do not\", data)\n",
    "        data = re.sub(r\"you're\", \"you are\", data)\n",
    "        data = re.sub(r\"i've\", \"I have\", data)\n",
    "        data = re.sub(r\"that's\", \"that is\", data)\n",
    "        data = re.sub(r\"i'll\", \"I will\", data)\n",
    "        data = re.sub(r\"doesn't\", \"does not\",data)\n",
    "        data = re.sub(r\"i'd\", \"I would\", data)\n",
    "        data = re.sub(r\"didn't\", \"did not\", data)\n",
    "        data = re.sub(r\"ain't\", \"am not\", data)\n",
    "        data = re.sub(r\"you'll\", \"you will\", data)\n",
    "        data = re.sub(r\"I've\", \"I have\", data)\n",
    "        data = re.sub(r\"Don't\", \"do not\", data)\n",
    "        data = re.sub(r\"I'll\", \"I will\", data)\n",
    "        data = re.sub(r\"I'd\", \"I would\", data)\n",
    "        data = re.sub(r\"Let's\", \"Let us\", data)\n",
    "        data = re.sub(r\"you'd\", \"You would\", data)\n",
    "        data = re.sub(r\"It's\", \"It is\", data)\n",
    "        data = re.sub(r\"Ain't\", \"am not\", data)\n",
    "        data = re.sub(r\"Haven't\", \"Have not\", data)\n",
    "        data = re.sub(r\"Could've\", \"Could have\", data)\n",
    "        data = re.sub(r\"youve\", \"you have\", data)  \n",
    "        data = re.sub(r\"donå«t\", \"do not\", data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning Urls\n",
    "    def remove_urls(data):\n",
    "        clean_url_regex = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "        data = clean_url_regex.sub(r\"\", data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning noisy data\n",
    "    def remove_noisy_char(data):\n",
    "        data = data.replace(\"\\\\r\", \"\").strip()\n",
    "        return data\n",
    "    \n",
    "    #cleaning HTML tags\n",
    "    def remove_HTML_tags(data):\n",
    "        soup = BeautifulSoup(data, 'html.parser') \n",
    "        return soup.get_text()\n",
    "        \n",
    "        \n",
    "    #cleaning emojis   \n",
    "    def remove_emojis(data):\n",
    "        emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        data = emoji_clean.sub(r\"\",data)\n",
    "        return data\n",
    "    \n",
    "    #cleaning unicode characters\n",
    "    def remove_unicode_chars(data):\n",
    "        data = (data.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning punctuations\n",
    "    def remove_punctuations(data):\n",
    "        #clean_punct_regex = re.compile(r\"[^\\w\\s\\d]+\")\n",
    "        clean_punct_regex = re.compile(r\"[^a-zA-Z0-9\\s]+\")\n",
    "        data = clean_punct_regex.sub(r\" \", data)\n",
    "                        \n",
    "        #credits - https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "        #data = data.translate(str.maketrans('', '', string.punctuation))   \n",
    "        return data\n",
    "    \n",
    "    #cleaning numeric characters\n",
    "    def remove_numerics(data):\n",
    "        #clean_num_regex = re.compile(r\"[^A-Za-z]+\")\n",
    "        #data = clean_num_regex.sub(r\" \", data)\n",
    "        #clean_alphanum_regex = re.compile(r\"\\S*\\d\\S*\")\n",
    "        #data = clean_alphanum_regex.sub(r\"\", data)\n",
    "        \n",
    "        clean_num_regex = re.compile(r\"\\b[0-9]+\\b\")\n",
    "        data = clean_num_regex.sub(r\"\", data)\n",
    "        return data\n",
    "    \n",
    "    def remove_single_chars(data):\n",
    "        #credits - https://stackoverflow.com/questions/42066352/python-regex-to-replace-all-single-word-characters-in-string\n",
    "        clean_single_len_regex = re.compile(r\"\\b[a-zA-Z]\\b\")\n",
    "        data = clean_single_len_regex.sub(r\"\", data)\n",
    "        return data\n",
    "    \n",
    "    #cleaning unwanted whitespaces\n",
    "    def remove_redundant_whiteSpaces(data):\n",
    "        clean_redundant_whitespaces_regex = re.compile(r\"\\s\\s+\") #check for more consecutive spaces\n",
    "        data = clean_redundant_whitespaces_regex.sub(r\" \", data) #replace with single space\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning long length words (greater than 30 chars)\n",
    "    def remove_long_length_tokens(data):\n",
    "        data = ' '.join(word.lower() for word in data.split() if len(word) <= 30)\n",
    "        data = data.strip()\n",
    "        return data\n",
    "    \n",
    "    df[clean_col]= df[col].apply(general)\n",
    "    df[clean_col]= df[clean_col].apply(remove_contractions)\n",
    "    df[clean_col]= df[clean_col].apply(remove_urls)\n",
    "    df[clean_col]= df[clean_col].apply(remove_noisy_char)\n",
    "    df[clean_col]= df[clean_col].apply(remove_HTML_tags)\n",
    "    df[clean_col]= df[clean_col].apply(remove_emojis)\n",
    "    df[clean_col]= df[clean_col].apply(remove_unicode_chars)\n",
    "    df[clean_col]= df[clean_col].apply(remove_punctuations)\n",
    "    df[clean_col]= df[clean_col].apply(remove_numerics)\n",
    "    df[clean_col]= df[clean_col].apply(remove_single_chars)\n",
    "    df[clean_col]= df[clean_col].apply(remove_redundant_whiteSpaces)\n",
    "    df[clean_col]= df[clean_col].apply(remove_long_length_tokens)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_json('embold_train.json').reset_index(drop=True)\n",
    "data_frame['text'] = data_frame.title + ' ' + data_frame.body\n",
    "\n",
    "train_extra_df = pd.read_json('embold_train_extra.json').reset_index(drop=True)\n",
    "train_extra_df['text'] = train_extra_df.title + ' ' + train_extra_df.body\n",
    "\n",
    "'''\n",
    "test_df = pd.read_json('embold_test.json').reset_index(drop=True)\n",
    "test_df['text'] = test_df.title + ' ' + test_df.body\n",
    "test_df = clean_text(test_df, 'text', 'clean_text')\n",
    "'''\n",
    "\n",
    "#data_frame = pd.concat([data_frame,train_extra_df],ignore_index=True)\n",
    "data_frame = clean_text(data_frame, 'text', 'clean_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "batchsize = 128\n",
    "\n",
    "label = data_frame['label'].values\n",
    "datatext = data_frame['clean_text'].values\n",
    "\n",
    "num_examples = len(label)\n",
    "\n",
    "train_end = int(train_ratio * num_examples)\n",
    "validation_end = train_end + int(validation_ratio * num_examples)\n",
    "\n",
    "train_input = datatext[:train_end]\n",
    "train_target = label[:train_end]\n",
    "\n",
    "validation_input = datatext[train_end:validation_end]\n",
    "validation_target = label[train_end:validation_end]\n",
    "\n",
    "test_input = datatext[validation_end:]\n",
    "test_target = label[validation_end:]\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_input,train_target)).batch(batchsize)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((validation_input,validation_target)).batch(batchsize)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_input,test_target)).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    \n",
    ")\n",
    "text_vectorization.adapt(train_input)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2110/2110 [==============================] - 103s 48ms/step - loss: -0.1282 - accuracy: 0.7097 - val_loss: -0.7154 - val_accuracy: 0.7475\n",
      "Epoch 2/2\n",
      "2110/2110 [==============================] - 92s 44ms/step - loss: -1.1250 - accuracy: 0.7078 - val_loss: -1.6959 - val_accuracy: 0.7526\n",
      "704/704 [==============================] - 12s 17ms/step - loss: -1.5485 - accuracy: 0.7575\n",
      "Test acc: 0.758\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(15))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_format=\"h5\")\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=2, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
