{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yy2kzBWDicgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab0da6f-8ff1-47f9-9800-43842a960656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  Installing build dependencies ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Getting requirements to build wheel ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Preparing metadata (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/950.8 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m122.9/950.8 kB\\x1b[0m \\x1b[31m3.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m942.1/950.8 kB\\x1b[0m \\x1b[31m13.6 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m950.8/950.8 kB\\x1b[0m \\x1b[31m11.0 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25h\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/6.5 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m3.9/6.5 MB\\x1b[0m \\x1b[31m98.2 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m6.5/6.5 MB\\x1b[0m \\x1b[31m103.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m6.5/6.5 MB\\x1b[0m \\x1b[31m103.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m6.5/6.5 MB\\x1b[0m \\x1b[31m54.0 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25h  Building wheel for keras-nlp (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!!pip install -q rouge-score\n",
        "!!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lXyLkwE-icgG"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "import pathlib\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrEljXYFicgG"
      },
      "source": [
        "Let's also define our parameters/hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "127_M-djicgG"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10 # This should be at least 10 for convergence\n",
        "MAX_SEQUENCE_LENGTH = 5\n",
        "NUMBERS_SIZE = 1000\n",
        "WORDS_SIZE = 1000\n",
        "\n",
        "EMBED_DIM = 128\n",
        "INTERMEDIATE_DIM = 1024\n",
        "NUM_HEADS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJjkDAcoicgH"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the pairs"
      ],
      "metadata": {
        "id": "Fj5w4Z-l1ku7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lists for digit names\n",
        "digit_names = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "tens_names = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
        "teen_names = [\"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"]\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "for num in range(1000): #0-999\n",
        "    #for 100-999, hundreds word value = {floor of num/100 (ith value in digit array)} OR {\"\" if number is not 100-999}\n",
        "    hundreds = (digit_names[num//100] + \" hundred \") if (num >= 100) else \"\"\n",
        "    hundredsdigit = str(num//100) + \" \" if num >= 100 else \"\"\n",
        "    remainder = num % 100 #gets all other numbers in 0-99\n",
        "\n",
        "    if remainder == 0: #taking care of 0 separately for easier if else conditions\n",
        "        text_pairs.append((\"0\", \"zero\"))\n",
        "    else:\n",
        "        if remainder < 10: #1-9\n",
        "            ones = digit_names[remainder] #ith value in digit array -> ones word value\n",
        "            text_pairs.append((hundredsdigit + \"0 \" + str(remainder), hundreds + ones)) #adding tuple to text_pairs\n",
        "        elif remainder < 20: #10-19 (the teens)\n",
        "            teens = teen_names[remainder % 10] #gets remainder of number/10 to get the ith value in teens array -> teens word value\n",
        "            text_pairs.append((hundredsdigit +str(remainder)[0] + \" \" + str(remainder)[1], hundreds + teens)) #adding tuple to text_pairs\n",
        "        else: #20-99\n",
        "            tens = tens_names[remainder // 10] #floor of number/10 = ith value in tens -> tens word value\n",
        "            ones = (\" \" + digit_names[remainder % 10]) if (remainder % 10) > 0 else \"\" #remainder of number/10 = ith value in digit array -> ones word value\n",
        "            text_pairs.append((hundredsdigit + str(remainder)[0] + \" \" + str(remainder)[1],hundreds +  tens + ones)) #adding tuple to text_pairs\n"
      ],
      "metadata": {
        "id": "jwQ8iw6i1cq1"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-KZTd_IicgI"
      },
      "source": [
        "Here's what our number pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "-P2qEbhAicgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a31159-cf10-4a27-b953-0050d9c341df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('2 5 5', 'two hundred fifty five')\n",
            "('0 8', 'eight')\n",
            "('9 4 6', 'nine hundred forty six')\n",
            "('2 4 2', 'two hundred forty two')\n",
            "('6 2 9', 'six hundred twenty nine')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed3NnQuicgI"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6w1QaGDRicgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2e2b48-17b2-4804-be62-7846e5c154d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 total pairs\n",
            "700 training pairs\n",
            "150 validation pairs\n",
            "150 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWBQC7picgJ"
      },
      "source": [
        "## Tokenizing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "iA7CjgZyicgJ"
      },
      "outputs": [],
      "source": [
        "def train_word_piece(the_samples, the_size, reserved_tokens):\n",
        "    tokens = tf.data.Dataset.from_tensor_slices(the_samples)\n",
        "    number = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        tokens.batch(1000).prefetch(2),\n",
        "        vocabulary_size=the_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return number\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Qi7fCJZCicgJ"
      },
      "outputs": [],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "num_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "num_lib = train_word_piece(num_samples, NUMBERS_SIZE, reserved_tokens)\n",
        "\n",
        "words_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "words_lib = train_word_piece(words_samples, WORDS_SIZE, reserved_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "z1tL9irYicgK"
      },
      "outputs": [],
      "source": [
        "num_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=num_lib, lowercase=False\n",
        ")\n",
        "word_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=words_lib, lowercase=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwyfRugkicgK"
      },
      "source": [
        "Let's try and tokenize a sample from our dataset! To verify whether the text has\n",
        "been tokenized correctly, we can also detokenize the list of tokens back to the\n",
        "original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "spacpnjCicgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b893ca4-4029-40b9-e8e2-c1232caaf344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number:  4 5 9\n",
            "Tokens:  tf.Tensor([ 8  9 13], shape=(3,), dtype=int32)\n",
            "\n",
            "Number in words:  four hundred fifty nine\n",
            "Tokens:  tf.Tensor([31 22 33 29], shape=(4,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "num_input_ex = text_pairs[3][0]\n",
        "num_tokens_ex = num_tokenizer.tokenize(num_input_ex)\n",
        "print(\"Number: \", num_input_ex)\n",
        "print(\"Tokens: \", num_tokens_ex)\n",
        "print()\n",
        "\n",
        "words_input_ex = text_pairs[3][1]\n",
        "words_tokens_ex = word_tokenizer.tokenize(words_input_ex)\n",
        "print(\"Number in words: \", words_input_ex)\n",
        "print(\"Tokens: \", words_tokens_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXdQF9LhicgK"
      },
      "source": [
        "## Format datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "nFMctGPUicgK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_batch(digits, words):\n",
        "    batch_size = tf.shape(words)[0]\n",
        "\n",
        "    digits = num_tokenizer(digits)\n",
        "    words = word_tokenizer(words)\n",
        "\n",
        "    # Pad `digits` to `MAX_SEQUENCE_LENGTH`.\n",
        "    digits_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=num_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    digits = digits_start_end_packer(digits)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `words` and pad it as well.\n",
        "    words_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=word_tokenizer.token_to_id(\"[START]\"),\n",
        "        end_value=word_tokenizer.token_to_id(\"[END]\"),\n",
        "        pad_value=word_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    words = words_start_end_packer(words)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": digits,\n",
        "            \"decoder_inputs\": words[:, :-1],\n",
        "        },\n",
        "        words[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    digits_texts, words_texts = zip(*pairs)\n",
        "    digits_texts = list(digits_texts)\n",
        "    words_texts = list(words_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((digits_texts, words_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yixh_MuzicgK"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 40 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "OM_our2ticgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7882a0-76a1-45ef-ffbc-0c3f9a4b79cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (32, 5)\n",
            "inputs[\"decoder_inputs\"].shape: (32, 5)\n",
            "targets.shape: (32, 5)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccbRvQnwicgL"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "YyppeuuMicgL"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=NUMBERS_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=WORDS_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(WORDS_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgMfajvgicgL"
      },
      "source": [
        "## Training our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "qS0l-bo2icgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb24c8e7-d17c-4b2d-9af1-72151d4ea5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 11s 90ms/step - loss: 0.0541 - accuracy: 0.9877 - val_loss: 0.0080 - val_accuracy: 0.9973\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 1s 49ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 1s 49ms/step - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9986\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 2s 76ms/step - loss: 0.0112 - accuracy: 0.9977 - val_loss: 0.0042 - val_accuracy: 0.9986\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 1s 46ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 9.9008e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 1s 51ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 5.7612e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 1s 48ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0063 - val_accuracy: 0.9986\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 1s 46ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9986\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 1s 52ms/step - loss: 0.0279 - accuracy: 0.9942 - val_loss: 0.0077 - val_accuracy: 0.9986\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f210d3984c0>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}